{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0daf71da",
   "metadata": {},
   "source": [
    "# 06 Texte auswerten mit dem Computer\n",
    "\n",
    "Ein wenig rechnen mit Wörtern: Welche Stichwörter beschreiben einen Text am besten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a4cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eine': 4, 'das': 4, 'was': 3, 'ist': 3, 'nächste': 3, 'chatgpt': 2, 'ein': 2, 'die': 2, 'ki': 2, 'sie': 2, 'einer': 2, 'und': 2, 'wieder': 2, 'etc': 1, 'eigentlich': 1, 'benutzeroberfläche': 1, 'unter': 1, 'der': 1, 'sich': 1, 'so': 1, 'genanntes': 1, 'generatives': 1, 'sprachmodell': 1, 'verbirgt': 1, 'eigentliche': 1, 'maschine': 1, 'wohlgeformte': 1, 'texte': 1, 'produzieren': 1, 'kann': 1, 'hat': 1, 'aus': 1, 'enormen': 1, 'menge': 1, 'von': 1, 'menschengemachten': 1, 'texten': 1, 'gelernt': 1, 'wie': 1, 'würden': 1, 'menschen': 1, 'in': 1, 'vergleichbaren': 1, 'situation': 1, 'wohl': 1, 'wort': 1, 'wählen': 1, 'dann': 1, 'nicht': 1, 'wahrheitsmaschine': 1, 'weltgeist': 1, 'oder': 1, 'echte': 1, 'künstliche': 1, 'intelligenz': 1, 'im': 1, 'sinne': 1, 'dessen': 1, 'forscher': 1, 'innen': 1, 'erst': 1, 'noch': 1, 'erreichen': 1, 'wollen': 1, 'keine': 1, 'agi': 1, 'artificial': 1, 'general': 1, 'intelligence': 1}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "    \n",
    "def bag_of_words(text):\n",
    "    # Convert to lowercase and extract words\n",
    "    \n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Count frequencies and return as dict sorted by frequency\n",
    "    word_counts = Counter(words)\n",
    "    return dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "text1 = \"\"\"\n",
    "Was chatGPT (etc.) eigentlich ist: eine Benutzeroberfläche, unter der sich ein so \n",
    "genanntes “Generatives Sprachmodell” verbirgt, die eigentliche KI: eine Maschine, \n",
    "die wohlgeformte Texte produzieren kann. Das hat sie aus einer enormen Menge von \n",
    "menschengemachten Texten gelernt: Wie würden Menschen in einer vergleichbaren \n",
    "Situation wohl das nächste Wort wählen? Und dann wieder das nächste, und wieder \n",
    "das nächste.  \n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Was sie nicht ist: Eine Wahrheitsmaschine, ein Weltgeist oder eine echte \n",
    "“Künstliche Intelligenz” im Sinne dessen, was KI-Forscher:innen erst noch \n",
    "erreichen wollen: chatGPT ist keine  “AGI” (“Artificial General Intelligence”). \n",
    "\"\"\"\n",
    "\n",
    "print(bag_of_words(text1 + text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802fd38",
   "metadata": {},
   "source": [
    "**Jetzt du:** Lass dir ein Programm schreiben, das die Häufigkeit der einzelnen Wörter angibt: Für ```text1```, für ```text2``` - und für beide Texte zusammen. \n",
    "\n",
    "## Topic Modeling: Die richtigen Schlagworte extrahieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_of_words(text1))\n",
    "print(bag_of_words(text2))\n",
    "print(bag_of_words(text1 + text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662655e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tf_idf_keywords(text, threshold=4):\n",
    "    # Simple TF-IDF implementation\n",
    " \n",
    "    \n",
    "    # Tokenize and normalize\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Calculate term frequency\n",
    "    tf = Counter(words)\n",
    "    \n",
    "    # Calculate document frequency\n",
    "    df = Counter()\n",
    "    for doc in words:\n",
    "        doc_words = set(re.findall(r'\\w+', doc.lower()))\n",
    "        for word in doc_words:\n",
    "            df[word] += 1\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    tf_idf = {}\n",
    "    N = len(words)\n",
    "    \n",
    "    for word, frequency in tf.items():\n",
    "        if word in df:\n",
    "            idf = math.log(N / df[word])\n",
    "            val = frequency * idf\n",
    "            if val >= threshold:\n",
    "                tf_idf[word] = val\n",
    "    \n",
    "    # Return sorted keywords by weight\n",
    "    return dict(sorted(tf_idf.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de2aea",
   "metadata": {},
   "source": [
    "**Jetzt du**: Wende die tf_idf Funktion auf text1 und text2 an - und vergleiche mit dem Bag of words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c68cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eine': 4, 'das': 4, 'was': 3, 'ist': 3, 'nächste': 3, 'chatgpt': 2, 'ein': 2, 'die': 2, 'ki': 2, 'sie': 2, 'einer': 2, 'und': 2, 'wieder': 2, 'etc': 1, 'eigentlich': 1, 'benutzeroberfläche': 1, 'unter': 1, 'der': 1, 'sich': 1, 'so': 1, 'genanntes': 1, 'generatives': 1, 'sprachmodell': 1, 'verbirgt': 1, 'eigentliche': 1, 'maschine': 1, 'wohlgeformte': 1, 'texte': 1, 'produzieren': 1, 'kann': 1, 'hat': 1, 'aus': 1, 'enormen': 1, 'menge': 1, 'von': 1, 'menschengemachten': 1, 'texten': 1, 'gelernt': 1, 'wie': 1, 'würden': 1, 'menschen': 1, 'in': 1, 'vergleichbaren': 1, 'situation': 1, 'wohl': 1, 'wort': 1, 'wählen': 1, 'dann': 1, 'nicht': 1, 'wahrheitsmaschine': 1, 'weltgeist': 1, 'oder': 1, 'echte': 1, 'künstliche': 1, 'intelligenz': 1, 'im': 1, 'sinne': 1, 'dessen': 1, 'forscher': 1, 'innen': 1, 'erst': 1, 'noch': 1, 'erreichen': 1, 'wollen': 1, 'keine': 1, 'agi': 1, 'artificial': 1, 'general': 1, 'intelligence': 1}\n",
      "{'eine': 12.409368034448997, 'das': 12.409368034448997, 'was': 10.17007224319209, 'ist': 10.17007224319209, 'nächste': 10.17007224319209, 'chatgpt': 7.590978378344389, 'ein': 7.590978378344389, 'die': 7.590978378344389, 'ki': 7.590978378344389, 'sie': 7.590978378344389, 'einer': 7.590978378344389, 'und': 7.590978378344389, 'wieder': 7.590978378344389, 'etc': 4.48863636973214, 'eigentlich': 4.48863636973214, 'benutzeroberfläche': 4.48863636973214, 'unter': 4.48863636973214, 'der': 4.48863636973214, 'sich': 4.48863636973214, 'so': 4.48863636973214, 'genanntes': 4.48863636973214, 'generatives': 4.48863636973214, 'sprachmodell': 4.48863636973214, 'verbirgt': 4.48863636973214, 'eigentliche': 4.48863636973214, 'maschine': 4.48863636973214, 'wohlgeformte': 4.48863636973214, 'texte': 4.48863636973214, 'produzieren': 4.48863636973214, 'kann': 4.48863636973214, 'hat': 4.48863636973214, 'aus': 4.48863636973214, 'enormen': 4.48863636973214, 'menge': 4.48863636973214, 'von': 4.48863636973214, 'menschengemachten': 4.48863636973214, 'texten': 4.48863636973214, 'gelernt': 4.48863636973214, 'wie': 4.48863636973214, 'würden': 4.48863636973214, 'menschen': 4.48863636973214, 'in': 4.48863636973214, 'vergleichbaren': 4.48863636973214, 'situation': 4.48863636973214, 'wohl': 4.48863636973214, 'wort': 4.48863636973214, 'wählen': 4.48863636973214, 'dann': 4.48863636973214, 'nicht': 4.48863636973214, 'wahrheitsmaschine': 4.48863636973214, 'weltgeist': 4.48863636973214, 'oder': 4.48863636973214, 'echte': 4.48863636973214, 'künstliche': 4.48863636973214, 'intelligenz': 4.48863636973214, 'im': 4.48863636973214, 'sinne': 4.48863636973214, 'dessen': 4.48863636973214, 'forscher': 4.48863636973214, 'innen': 4.48863636973214, 'erst': 4.48863636973214, 'noch': 4.48863636973214, 'erreichen': 4.48863636973214, 'wollen': 4.48863636973214, 'keine': 4.48863636973214, 'agi': 4.48863636973214, 'artificial': 4.48863636973214, 'general': 4.48863636973214, 'intelligence': 4.48863636973214}\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words(text1 + text2))\n",
    "print(tf_idf_keywords(text1 + text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203d009",
   "metadata": {},
   "source": [
    "Okay... aber zu viel Müll: Wir müssen: \n",
    "- die **Stoppwörter** rausnehmen (ein, die, und...)\n",
    "- die Wortstämme (Lemmata) extrahieren\n",
    "\n",
    "Das machen wir mit einer Spezial-Bibliothek (die auch schon ein wenig KI benutzt): Spacy. \n",
    "\n",
    "Außerdem benutzen wir die Tabellen-Bibliothek pandas - und wir geben ihr einen anderen Namen. Details sind nicht so wichtig; einfach schauen: ähnliche Code-Schnipsel sieht man sehr oft!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5afa7ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (2.3.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
      "Requirement already satisfied: wrapt in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Requirement already satisfied: scikit.learn in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from scikit.learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from scikit.learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from scikit.learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from scikit.learn) (3.6.0)\n",
      "Requirement already satisfied: pandas in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install scikit.learn\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7e4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\n",
      "Was chatGPT (etc.) eigentlich ist: eine Benutzeroberfläche, unter der sich ein so \n",
      "genanntes “Generatives Sprachmodell” verbirgt, die eigentliche KI: eine Maschine, \n",
      "die wohlgeformte Texte produzieren kann. Das hat sie aus einer enormen Menge von \n",
      "menschengemachten Texten gelernt: Wie würden Menschen in einer vergleichbaren \n",
      "Situation wohl das nächste Wort wählen? Und dann wieder das nächste, und wieder \n",
      "das nächste.  \n",
      "\n",
      "Was sie nicht ist: Eine Wahrheitsmaschine, ein Weltgeist oder eine echte \n",
      "“Künstliche Intelligenz” im Sinne dessen, was KI-Forscher:innen erst noch \n",
      "erreichen wollen: chatGPT ist keine  “AGI” (“Artificial General Intelligence”). \n",
      "\n",
      "\n",
      "Lemmatized text:\n",
      "\n",
      " chatGPT etc. eigentlich Benutzeroberfläche \n",
      " genannt Generativ Sprachmodell verbirgen eigentlich KI Maschine \n",
      " wohlgeformt Text produzieren enorm Menge \n",
      " menschengemachten Text lernen Mensch vergleichbar \n",
      " Situation nächster Wort wählen nächster \n",
      " nächster  \n",
      "\n",
      " Wahrheitsmaschine Weltgeist echt \n",
      " Künstliche Intelligenz Sinn KI-Forscher innen \n",
      " erreichen chatGPT   AGI Artificial General Intelligence \n",
      "\n",
      "\n",
      "TF-IDF scores:\n",
      "        agi  artificial  benutzeroberfläche   chatgpt      echt  eigentlich  \\\n",
      "0  0.133631    0.133631            0.133631  0.267261  0.133631    0.267261   \n",
      "\n",
      "      enorm  erreichen       etc  forscher  ...  situation  sprachmodell  \\\n",
      "0  0.133631   0.133631  0.133631  0.133631  ...   0.133631      0.133631   \n",
      "\n",
      "       text  verbirgen  vergleichbar  wahrheitsmaschine  weltgeist  \\\n",
      "0  0.267261   0.133631      0.133631           0.133631   0.133631   \n",
      "\n",
      "   wohlgeformt      wort    wählen  \n",
      "0     0.133631  0.133631  0.133631  \n",
      "\n",
      "[1 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.de.examples import sentences \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.cli import download\n",
    "import pandas as pd\n",
    "\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        download(model_name)         # lädt das passende Paket ins aktuelle Python-Umfeld\n",
    "        nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = load_model(\"de_core_news_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = text1 + text2 \n",
    "\n",
    "# Lemmatization using spaCy\n",
    "doc = nlp(text)\n",
    "lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized_text)\n",
    "\n",
    "# TF-IDF calculation\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([lemmatized_text])\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(\"\\nTF-IDF scores:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9466cb",
   "metadata": {},
   "source": [
    "Das ist schon sehr beeindruckend, aber noch vergleichsweise naiv: Wäre es nicht schöner, wenn wir statt dessen eine KI fragen, welches die wichtigsten Wörter in einem Text sind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31060d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c69101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "model = \"mistral-medium-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def ask(prompt):\n",
    "    chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return chat_response.choices[0].message.content   \n",
    "\n",
    "\n",
    "# Hier ergänzen: Das Modell soll selbst Keywords extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823ebc3",
   "metadata": {},
   "source": [
    "## Audio verschriftlichen\n",
    "\n",
    "...und es wird noch cooler! Mit der richtigen Bibliothek kann man auch Audio in Text verwandeln: **Whisper** von OpenAI wurde ursprünglich dazu entwickelt, um (vor allem) Youtube-Videos zu transkribieren. Und setzt seitdem den Standard für saubere Transkription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03675c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erst die Bibliothek holen\n",
    "!pip install faster_whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "\n",
    "model = WhisperModel(\n",
    "    \"deepdml/faster-whisper-large-v3-turbo-ct2\",  # CT2 model id\n",
    "    device=\"cpu\",                 # or \"cuda\"\n",
    "    compute_type=\"int8\",          # good for CPU; try \"int8_float16\" on CUDA\n",
    ")\n",
    "\n",
    "path=\"./daten/demoaudio.mp3\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"{path} gibt es leider nicht.\")\n",
    "    raise\n",
    "\n",
    "segments, info = model.transcribe(\n",
    "    path,\n",
    "    language=\"de\",                # force German\n",
    "    beam_size=5,\n",
    "    temperature=0.0,\n",
    "    vad_filter=True,              # optional: basic VAD\n",
    ")\n",
    "\n",
    "text = \"\".join(s.text for s in segments)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849d883",
   "metadata": {},
   "source": [
    "**Jetzt du**: Schreib eine Funktion, die Audios in Text umwandeln kann. Übergib der Funktion den Pfad zu einer Audio-Datei, die du in Text verwandeln möchtest. \n",
    "\n",
    "TIPP: Whisper kann nicht mit allen Dateitypen umgehen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b7fb2",
   "metadata": {},
   "source": [
    "## Bild beschreiben\n",
    "\n",
    "Moderne Sprachmodelle verstehen nicht nur Text, sondern sind **multimodal** - das heißt, man kann ihnen auch Bilddateien als Eingabe geben. \n",
    "\n",
    "Kannst du mit der Funktion hier einen Bildbeschreiber bauen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93438f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Generation function\n",
    "def generate_response(prompt ='', \n",
    "                      imageprompt = None,\n",
    "                      model='gpt-4o',\n",
    "                      client = None):\n",
    "    base64_image = imageprompt.split(',')[1] if imageprompt else None\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":  f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            }\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der API-Anfrage: {e}\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
