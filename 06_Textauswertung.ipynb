{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0daf71da",
   "metadata": {},
   "source": [
    "# 06 Texte auswerten mit dem Computer\n",
    "\n",
    "Ein wenig rechnen mit Wörtern: Welche Stichwörter beschreiben einen Text am besten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "    \n",
    "def bag_of_words(text):\n",
    "    # Convert to lowercase and extract words\n",
    "    \n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Count frequencies and return as dict sorted by frequency\n",
    "    word_counts = Counter(words)\n",
    "    return dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "text1 = \"\"\"\n",
    "Was chatGPT (etc.) eigentlich ist: eine Benutzeroberfläche, unter der sich ein so \n",
    "genanntes “Generatives Sprachmodell” verbirgt, die eigentliche KI: eine Maschine, \n",
    "die wohlgeformte Texte produzieren kann. Das hat sie aus einer enormen Menge von \n",
    "menschengemachten Texten gelernt: Wie würden Menschen in einer vergleichbaren \n",
    "Situation wohl das nächste Wort wählen? Und dann wieder das nächste, und wieder \n",
    "das nächste.  \n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Was sie nicht ist: Eine Wahrheitsmaschine, ein Weltgeist oder eine echte \n",
    "“Künstliche Intelligenz” im Sinne dessen, was KI-Forscher:innen erst noch \n",
    "erreichen wollen: chatGPT ist keine  “AGI” (“Artificial General Intelligence”). \n",
    "\"\"\"\n",
    "\n",
    "print(bag_of_words(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802fd38",
   "metadata": {},
   "source": [
    "**Jetzt du:** Lass dir ein Programm schreiben, das die Häufigkeit der einzelnen Wörter angibt: Für ```text1```, für ```text2``` - und für beide Texte zusammen. \n",
    "\n",
    "## Topic Modeling: Die richtigen Schlagworte extrahieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_of_words(text1))\n",
    "print(bag_of_words(text2))\n",
    "print(bag_of_words(text1 + text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662655e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tf_idf_keywords(text, threshold=4):\n",
    "    # Simple TF-IDF implementation\n",
    " \n",
    "    \n",
    "    # Tokenize and normalize\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    # Calculate term frequency\n",
    "    tf = Counter(words)\n",
    "    \n",
    "    # Calculate document frequency\n",
    "    df = Counter()\n",
    "    for doc in words:\n",
    "        doc_words = set(re.findall(r'\\w+', doc.lower()))\n",
    "        for word in doc_words:\n",
    "            df[word] += 1\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    tf_idf = {}\n",
    "    N = len(words)\n",
    "    \n",
    "    for word, frequency in tf.items():\n",
    "        if word in df:\n",
    "            idf = math.log(N / df[word])\n",
    "            val = frequency * idf\n",
    "            if val >= threshold:\n",
    "                tf_idf[word] = val\n",
    "    \n",
    "    # Return sorted keywords by weight\n",
    "    return dict(sorted(tf_idf.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de2aea",
   "metadata": {},
   "source": [
    "**Jetzt du**: Wende die tf_idf Funktion auf text1 und text2 an - und vergleiche mit dem Bag of words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_of_words(text1 + text2))\n",
    "print(tf_idf_keywords(text1 + text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203d009",
   "metadata": {},
   "source": [
    "Okay... aber zu viel Müll: Wir müssen: \n",
    "- die **Stoppwörter** rausnehmen (ein, die, und...)\n",
    "- die Wortstämme (Lemmata) extrahieren\n",
    "\n",
    "Das machen wir mit einer Spezial-Bibliothek (die auch schon ein wenig KI benutzt): Spacy. \n",
    "\n",
    "Außerdem benutzen wir die Tabellen-Bibliothek pandas - und wir geben ihr einen anderen Namen. Details sind nicht so wichtig; einfach schauen: ähnliche Code-Schnipsel sieht man sehr oft!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install scikit.learn\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7e4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\n",
      "Was chatGPT (etc.) eigentlich ist: eine Benutzeroberfläche, unter der sich ein so \n",
      "genanntes “Generatives Sprachmodell” verbirgt, die eigentliche KI: eine Maschine, \n",
      "die wohlgeformte Texte produzieren kann. Das hat sie aus einer enormen Menge von \n",
      "menschengemachten Texten gelernt: Wie würden Menschen in einer vergleichbaren \n",
      "Situation wohl das nächste Wort wählen? Und dann wieder das nächste, und wieder \n",
      "das nächste.  \n",
      "\n",
      "Was sie nicht ist: Eine Wahrheitsmaschine, ein Weltgeist oder eine echte \n",
      "“Künstliche Intelligenz” im Sinne dessen, was KI-Forscher:innen erst noch \n",
      "erreichen wollen: chatGPT ist keine  “AGI” (“Artificial General Intelligence”). \n",
      "\n",
      "\n",
      "Lemmatized text:\n",
      "\n",
      " chatGPT etc. eigentlich Benutzeroberfläche \n",
      " genannt Generativ Sprachmodell verbirgen eigentlich KI Maschine \n",
      " wohlgeformt Text produzieren enorm Menge \n",
      " menschengemachten Text lernen Mensch vergleichbar \n",
      " Situation nächster Wort wählen nächster \n",
      " nächster  \n",
      "\n",
      " Wahrheitsmaschine Weltgeist echt \n",
      " Künstliche Intelligenz Sinn KI-Forscher innen \n",
      " erreichen chatGPT   AGI Artificial General Intelligence \n",
      "\n",
      "\n",
      "TF-IDF scores:\n",
      "        agi  artificial  benutzeroberfläche   chatgpt      echt  eigentlich  \\\n",
      "0  0.133631    0.133631            0.133631  0.267261  0.133631    0.267261   \n",
      "\n",
      "      enorm  erreichen       etc  forscher  ...  situation  sprachmodell  \\\n",
      "0  0.133631   0.133631  0.133631  0.133631  ...   0.133631      0.133631   \n",
      "\n",
      "       text  verbirgen  vergleichbar  wahrheitsmaschine  weltgeist  \\\n",
      "0  0.267261   0.133631      0.133631           0.133631   0.133631   \n",
      "\n",
      "   wohlgeformt      wort    wählen  \n",
      "0     0.133631  0.133631  0.133631  \n",
      "\n",
      "[1 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.de.examples import sentences \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.cli import download\n",
    "import pandas as pd\n",
    "\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        nlp = spacy.load(model_name)\n",
    "    except OSError:\n",
    "        download(model_name)         # lädt das passende Paket ins aktuelle Python-Umfeld\n",
    "        nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = load_model(\"de_core_news_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = text1 + text2 \n",
    "\n",
    "# Lemmatization using spaCy\n",
    "doc = nlp(text)\n",
    "lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized_text)\n",
    "\n",
    "# TF-IDF calculation\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([lemmatized_text])\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(\"\\nTF-IDF scores:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9466cb",
   "metadata": {},
   "source": [
    "Das ist schon sehr beeindruckend, aber noch vergleichsweise naiv: Wäre es nicht schöner, wenn wir statt dessen eine KI fragen, welches die wichtigsten Wörter in einem Text sind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31060d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistralai\n",
      "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpx>=0.28.1 (from mistralai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai)\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from mistralai) (2.12.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from mistralai) (2.9.0.post0)\n",
      "Collecting pyyaml<7.0.0,>=6.0.2 (from mistralai)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from mistralai) (0.4.2)\n",
      "Collecting anyio (from httpx>=0.28.1->mistralai)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from httpx>=0.28.1->mistralai) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.28.1->mistralai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from httpx>=0.28.1->mistralai) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.28.1->mistralai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pydantic>=2.10.3->mistralai) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from pydantic>=2.10.3->mistralai) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.28.1->mistralai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, pyyaml, invoke, h11, eval-type-backport, httpcore, anyio, httpx, mistralai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [mistralai]/9\u001b[0m [mistralai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anyio-4.11.0 eval-type-backport-0.2.2 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 invoke-2.2.1 mistralai-1.9.11 pyyaml-6.0.3 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c69101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "api_key = \"AVtuDWTw4B0dS70w3nssuwFUnkhQuU0c\"\n",
    "model = \"mistral-medium-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def ask(prompt):\n",
    "    chat_response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return chat_response.choices[0].message.content   \n",
    "\n",
    "\n",
    "# Hier ergänzen: Das Modell soll selbst Keywords extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823ebc3",
   "metadata": {},
   "source": [
    "## Audio verschriftlichen\n",
    "\n",
    "...und es wird noch cooler! Mit der richtigen Bibliothek kann man auch Audio in Text verwandeln: **Whisper** von OpenAI wurde ursprünglich dazu entwickelt, um (vor allem) Youtube-Videos zu transkribieren. Und setzt seitdem den Standard für saubere Transkription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f03675c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faster_whisper\n",
      "  Downloading faster_whisper-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting ctranslate2<5,>=4.0 (from faster_whisper)\n",
      "  Using cached ctranslate2-4.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub>=0.13 (from faster_whisper)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tokenizers<1,>=0.13 (from faster_whisper)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting onnxruntime<2,>=1.14 (from faster_whisper)\n",
      "  Downloading onnxruntime-1.23.2-cp312-cp312-macosx_13_0_arm64.whl.metadata (5.1 kB)\n",
      "Collecting av>=11 (from faster_whisper)\n",
      "  Downloading av-16.0.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from faster_whisper) (4.67.1)\n",
      "Requirement already satisfied: setuptools in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from ctranslate2<5,>=4.0->faster_whisper) (80.9.0)\n",
      "Requirement already satisfied: numpy in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from ctranslate2<5,>=4.0->faster_whisper) (2.3.4)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from ctranslate2<5,>=4.0->faster_whisper) (6.0.3)\n",
      "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster_whisper)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime<2,>=1.14->faster_whisper)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: packaging in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from onnxruntime<2,>=1.14->faster_whisper) (25.0)\n",
      "Collecting protobuf (from onnxruntime<2,>=1.14->faster_whisper)\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime<2,>=1.14->faster_whisper)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.13->faster_whisper)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.13->faster_whisper)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from huggingface-hub>=0.13->faster_whisper) (0.28.1)\n",
      "Requirement already satisfied: shellingham in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from huggingface-hub>=0.13->faster_whisper) (1.5.4)\n",
      "Collecting typer-slim (from huggingface-hub>=0.13->faster_whisper)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from huggingface-hub>=0.13->faster_whisper) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.13->faster_whisper)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: anyio in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.13->faster_whisper) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.13->faster_whisper) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.13->faster_whisper) (1.0.9)\n",
      "Requirement already satisfied: idna in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.13->faster_whisper) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.13->faster_whisper) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.13->faster_whisper) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster_whisper)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime<2,>=1.14->faster_whisper)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in ./vfm-Programmieren/.conda/lib/python3.12/site-packages (from typer-slim->huggingface-hub>=0.13->faster_whisper) (8.3.0)\n",
      "Downloading faster_whisper-1.2.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached ctranslate2-4.6.0-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Downloading onnxruntime-1.23.2-cp312-cp312-macosx_13_0_arm64.whl (17.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.0.1-py3-none-any.whl (503 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading av-16.0.1-cp312-cp312-macosx_14_0_arm64.whl (21.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: mpmath, flatbuffers, typer-slim, sympy, protobuf, humanfriendly, hf-xet, fsspec, filelock, ctranslate2, av, coloredlogs, onnxruntime, huggingface-hub, tokenizers, faster_whisper\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [faster_whisper]m [tokenizers]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed av-16.0.1 coloredlogs-15.0.1 ctranslate2-4.6.0 faster_whisper-1.2.0 filelock-3.20.0 flatbuffers-25.9.23 fsspec-2025.9.0 hf-xet-1.2.0 huggingface-hub-1.0.1 humanfriendly-10.0 mpmath-1.3.0 onnxruntime-1.23.2 protobuf-6.33.0 sympy-1.14.0 tokenizers-0.22.1 typer-slim-0.20.0\n"
     ]
    }
   ],
   "source": [
    "# Erst die Bibliothek holen\n",
    "!pip install faster_whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\n",
    "    \"deepdml/faster-whisper-large-v3-turbo-ct2\",  # CT2 model id\n",
    "    device=\"cpu\",                 # or \"cuda\"\n",
    "    compute_type=\"int8\",          # good for CPU; try \"int8_float16\" on CUDA\n",
    ")\n",
    "\n",
    "my_path=\"~/Downloads/demoaudio.mp3\"\n",
    "path = os.path.expanduser(my_path)\n",
    "print(os.path.exists(path))\n",
    "\n",
    "segments, info = model.transcribe(\n",
    "    path,\n",
    "    language=\"de\",                # force German\n",
    "    beam_size=5,\n",
    "    temperature=0.0,\n",
    "    vad_filter=True,              # optional: basic VAD\n",
    ")\n",
    "\n",
    "text = \"\".join(s.text for s in segments)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849d883",
   "metadata": {},
   "source": [
    "**Jetzt du**: Übergib der Funktion den Pfad zu einer Audio-Datei, die du in Text verwandeln möchtest. \n",
    "\n",
    "TIPP: Whisper kann nicht mit allen Dateitypen umgehen!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
